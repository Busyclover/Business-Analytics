---
title: "Kaggle Competition : Bike Sharing"
author: "Ming-Yu Liu"
date: "December 1, 2015"
output: 
  html_document: 
    highlight: haddock
    number_sections: yes
    theme: cerulean
    toc: yes
---

> Still far away from the leaderboard 

## Background 

This documentation is for the bike sharing Kaggle competition. The overall task is to use a hourly bike rental data spanning two years to train a machine learning algorithm and predict the total count of bikes rented during each hour covered by the test set. More detailed information, datasets and variables explanations can be found at this [link](https://www.kaggle.com/c/bike-sharing-demand).

We'll load in the training dataset and give a brief explanations of the variables that may not seem so intuitive.

```{r, message=FALSE, warning=FALSE}

library(dplyr)
library(tidyr)
library(caret)
library(rpart)
library(party)
library(scales)
library(ggplot2)
library(gridExtra)
library(lubridate)
library(data.table)
setwd("/Users/ethen/Business-Analytics/2_bike_sharing/data")

data <- fread("train.csv")
data

```

Dependent variables :

- `registered` Number of registered user.
- `casual` Number of non-registered user.
- `count` Number of total rentals (registered + casual).

Independent variables : 

- `temp` Temperature in Celsius.
- `atemp` "feels like" temperature in Celsius.
- `weather` Has four different values ranging from 1 to 4. 1 : Clear, 2 : Cloudy, 3 : Little rain, 4 : Heavy rain.
- `workingday` 1 indicates that the day is neither a weekend nor holiday and 0 otherwise, same goes for `holiday`.

## Exploratory Analysis

Before diving straight into machine learning algorithms, we'll perform some exploratory analysis to get to know the distribution of our data better. This allows us to :

- Check for missing values, and outliers, so that we can know whether they'll be needing transformations or included (excluded) for the modeling. 
- Discover the relationships between target (dependent) variables or predictors (independent), which can be used for feature selection and engineering.
- In our mind, there might be a lot of straightforward factors that may influence the demand of bikes, we can seize this chance to confirm if the patterns discovered in the data meets our original hypothesis. 

Let's start with a quick check to see if there's any NA value (missing value) for each column.

```{r}

sapply( data, function(y) sum( is.na(y) ) )

```

No missing values, good! Next we'll plot a histogram for all the numeric independent variables to visualize and understand its distribution. 

```{r, message = FALSE, warning = FALSE, fig.height = 7, fig.width = 9 }

independent <- c( "casual", "registered", "count" )
columns <- which( !names(data) %in% independent & sapply( data, is.numeric ) )

histogram <- lapply( names(columns), function(column)
{
	ggplot( data[ , column, with = FALSE ], aes_string( column ) ) + 
	geom_histogram()
})
do.call( grid.arrange, histogram )

```

From the visualization, we can see that :

- `season` Has four categories ranging from 1 to 4 that are equally distributed.
- `holiday` Consists mainly of 0s tells you most days are not considered as holidays.
- `weather` Shows that the weather for most of the days are clear (1).
- `workday` consists primarily of 1s tells you most days are neither weekends or holidays.
- Four variables including `temp`, `atemp`, `humidity` and `windspeed` are continuous variables and their values are not heavily skewed.

Third, we'll check the correlation between the continuous variables and the independent variables.

```{r}

cor( select( data, casual, registered, temp:windspeed ) ) %>%
findCorrelation( cutoff = .85, name = TRUE )

```

The result indicates that `atemp` is highly correlated with one of the other variables ( above the cutoff threshold .85 ), we can consider removing one of them from the model. You can check the correlation matrix, it'll show you that it is actually highly correlated with the `temp` variable.

Now that we've had the glimpse at our data, we'll start checking some hypotheses that comes into our mind. For the following hypotheses, we'll simualtaneously check to see if there's a difference in the patterns for registered users and non-registered users.

**1. Hourly trend: The demand for bikes should be higher for timespans that people get to and off work, shouldn't it?**

```{r, fig.height = 6, fig.width = 10 }

# convert datetime column to date type
data[ , datetime := ymd_hms(datetime) ]

# extract the datetime; and casual, registered column 
subdata1 <- select( data, datetime, casual, registered ) %>%
		    tbl_dt() %>%
		    mutate( datetime = hour(datetime) ) %>%
		    gather( "user", "count", -1 )

# box plot for registered and unregisted users
ggplot( subdata1, aes( datetime, count, group = datetime, fill = user ) ) + 
geom_boxplot() + 
facet_wrap( ~ user ) +
scale_fill_discrete( limit = c( "registered", "casual" ) ) + 
guides( fill = FALSE ) + 
ggtitle( "2011-2012 User's Hourly Bike Demand" )

```
 
To be clear, the "dots" at the end of the boxplot represent outliers, which is determined by checking if a data point is: less than Q1 - 1.5 * IQR or greater than Q3 + 1.5 * IQR, where IQR stands for interquartile range = Q3 - Q1. And the line goes to the first data point before the the cut-off. The fact that there're so many outliers shown in this boxplot infers that the `count` variable may requires some transformation (e.g. log transformation).

From the boxplot, clearly the *registered* users' ( boxes in red ) behavior can be sloppily categorized into three groups

- High: 7-9 and 17-19 hours.
- Average: 10-16 hours.
- Low: 0-6 and 20-24 hours.

**2. Daily trend: Is the demand for bikes higher on weekdays or weekends?**

```{r}

subdata2 <- select( data, datetime, casual, registered ) %>%
		    tbl_dt() %>%
		    mutate( datetime = lubridate::wday( datetime, label = TRUE ) ) %>%
		    gather( "user", "count", -1 ) %>%
		    group_by( datetime, user ) %>%
			summarise( count = sum(count) )

ggplot( subdata2, aes( datetime, count, group = user, color = user ) ) + 
geom_line( size = 1 ) + geom_point( size = 3 ) + 
scale_y_continuous( label = comma ) +
scale_colour_discrete( limit = c( "registered", "casual" ) ) + 
ggtitle( "2011-2012 User's Daily Bike Demand" )

```

The plot tells us that registered users' use bikes more often on weekdays, while non-registered users' behaviors are completely the opposite. Their demand increases over the weekend. 

**3. Trend of bike demand over year: Is the market size of bike sharing increasing or decreasing?** 

```{r, message=FALSE, warning=FALSE}

subdata3 <- select( data, datetime, casual, registered ) %>%
			tbl_dt() %>%
		    mutate( datetime = year(datetime) ) %>%
			gather( "user", "count", -1 ) %>% 
			group_by( datetime, user ) %>%
			summarise( count = sum(count) )

ggplot( subdata3, aes( datetime, count, fill = user ) ) + 
geom_bar( stat = "identity", position = "dodge", width = .6 ) + 
geom_text( aes( label = count ), position = position_dodge(.6), vjust = -.6, size = 3 ) +
scale_x_continuous( breaks = unique(subdata3$datetime) ) + 
scale_y_continuous( label = comma ) +
scale_fill_discrete( limit = c( "registered", "casual" ) ) +
ggtitle( "2011-2012 Bike Demand of Users" ) 

```

From the year 2011 to 2012, the demand for bike sharing is steadily increasing, especially for registered users.

**Section Conclusion:**

From the exploratory analysis section we can already tell that bike using behaviors of the registered and non-registered users are totally different.

## Feature Engineering 

To be specific, here's a list of what we'll be doing in the feature engineering section:

1. We know from the exploratory analysis section that bike demands will increase over time, and that Split the `datetime` variable into four variables. `hour`, `weekday`, `year` and `month`.
2. Combine `holiday` and `workingday` into one new variable indicating whether the day its a holiday, weekend or workday.
4. Log transformation of dependent variables, `casual` and `registered`.
5. Convert discrete variables into factors.

**Step 1:** 

- Add the hour, weekday, year, month column. 
- Split the original data into train / test set of 80 / 20 percent and remove the original dataset. This way we'll train the model using the training set and evaluate performance on test set. 
- Define the independent variables that will be used in the final model, things will be more clear after looking further down the documentation.

```{r}

data[ , c( "hour", "weekday", "year", "month" ) := list( hour(datetime), 
												   		 lubridate::wday(datetime),
												   		 year(datetime),
												   		 month(datetime) ) ]
# split 
set.seed(1234) 
train_index <- createDataPartition( data$count, p = .8, list = FALSE )
data_train  <- data[ train_index, ]
data_test   <- data[ -train_index, ]
rm(data)

# list of the final independent variables 
variables <- c( "weather", "temp", "humidity", "windspeed", 
				"type", "year", "month", "hour", "weekday" )

```

**Step 2 ~ 4:**

As for the rest of the steps, we'll first print out the [`ExtractFeatures`][ExtractFeatures] function that does the feature engineering for our dataset and explain along the way.

```{r}

ExtractFeatures <- function( data, data.type )
{

	# 2. convert holiday and workingday into one variable
	#    indicating whether its a holiday, weekend and workday
	data$type <- with( data, 

		ifelse( holiday == 1, "holiday",
		ifelse( holiday == 0 & workingday == 0, "weekend", "workday" ) ) 
	)

	# 3. keep the needed variables, and convert discrete variables into factors
	# if it is the original training data, log transformation of dependent variables 
	if( data.type == "train" )
	{
		columns <- which( colnames(data) %in% c( "casual", "registered", variables ) )
		data <- select( data, columns )

		# log transformation
		data <- transform( data, casual     = log( casual + 1 ),
					   		 	 registered = log( registered + 1 ) )

	}else # "test"
	{
		columns <- which( colnames(data) %in% variables )
		data <- select( data, columns )	
	}

	# 4. convert to factor 
	factor_col <- which( colnames(data) %in% 
						 c( "weather", "hour", "year", "month", "weekday", "type" ) )	

	data <- modifyList( data, lapply( select( data, factor_col ), as.factor ) )
			 
	return(data)
}

count <- data_test$count
data_train <- ExtractFeatures( data = data_train, data.type = "train" )
data_test  <- ExtractFeatures( data = data_test , data.type = "train" )

```



## Model Training

```{r, cache=TRUE}

# 10-fold cross validation 
control <- trainControl( method = "cv", number = 10 )

# evaluation measure 
RMSLE <- function( y, pred )
{
	sqrt( sum( ( log( y + 1) - log( pred + 1 ) )^2 ) / length(y) )
}

# formula 
formula_casual 	  <- as.formula( paste( "casual ~ ", paste( variables, collapse = " + " ) ) )
formula_registerd <- as.formula( paste( "registered ~ ", paste( variables, collapse = " + " ) ) )

model_ctree1 <- train( formula_casual, data 	 = data_train, 
									   trControl = control, 
									   method 	 = "ctree" )

model_ctree2 <- train( formula_registerd, data   	= data_train, 
									      trControl = control, 
									      method    = "ctree" )

result_ctree1 <- predict( model_ctree1, newdata = data_test )
result_ctree2 <- predict( model_ctree2, newdata = data_test )

predict_count <- ( exp(result_ctree1) - 1 ) + ( exp(result_ctree2) - 1 )
RMSLE( count, predict_count )

```



```{r}

# -------------------------------------------------------------------------
# predicting on the test set 
setwd("/Users/ethen/Business-Analytics/2_bike_sharing/data")
test1 <- fread("test.csv")
test1[ , datetime := ymd_hms(datetime) ]
test1[ , c( "hour", "weekday", "year", "month" ) := list( hour(datetime), 
												   		  lubridate::wday(datetime),
												   		  year(datetime),
												   		  month(datetime) ) ]
test2 <- ExtractFeatures( data = test1, data.type = "test" )

result1 <- predict( model_ctree1, newdata = test2 )
result2 <- predict( model_ctree2, newdata = test2 )

predict_count <- ( exp(result1) - 1 ) + ( exp(result2) - 1 )


result <- data.frame( datetime = test1$datetime, count = predict_count )
write.csv( result, "result.csv", row.names = FALSE )

```


## R session Information

```{r}

sessionInfo()

```


[ExtractFeatures]: 




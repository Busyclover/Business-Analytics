x <- setdiff( names(train), y )
?h2o.gbm
?activation
?h2o.deeplearning
model <- h2o.deeplearning(#
#
    training_frame = train,#
    x = x,#
    y = y,#
    validation_frame = test,#
    distribution = "multinomial",#
    activation = "RectifierWithDropout",#
    hidden = c( 32, 32, 32 ),#
    input_dropout_ratio = 0.2,#
    sparse = TRUE,#
    l1 = 1e-5,#
    epochs = 10#
)
??h2o.deeplearning
?h2o.deeplearning
model@parameters
h2o.performance( model, train = TRUE )
?h2o.performance
model@model
model@model$training_metrics
h2o.performance( model, train = TRUE )
h2o.performance(model, valid = TRUE)
model <- h2o.deeplearning(#
#
    training_frame = train,#
    x = x,#
    y = y,#
    validation_frame = test,#
    distribution = "multinomial",#
    activation = "RectifierWithDropout",#
    hidden = c( 32, 32, 32 ),#
    input_dropout_ratio = 0.2,#
    sparse = TRUE,#
    l1 = 1e-5,#
    epochs = 10,#
    variable_importances = TRUE#
)
h2o.varimp(model)
?h2o.grid
?h2o.randomForest
?h2o.grid
h2o.mse( model, valid = TRUE )
?h2o.mse
?h2o.deeplearning
hidden_opt <- list( c( 32, 32 ), c( 32, 16, 8 ), c(100) )#
l1_opt <- c( 1e-4, 1e-3 )#
hyper_params <- list( hidden = hidden_opt, l1 = l1_opt )#
#
model_grid <- h2o.grid(#
#
    algorithm = "deeplearning",#
    hyper_params = hyper_params,#
    x = x,#
    y = y,#
    distribution = "multinomial",#
    training_frame = train,#
    validation_frame = test,#
    score_interval = 2,#
    epochs = 1000,#
    stopping_rounds = 3,#
    stopping_tolerance = 0.05,#
    stopping_metric = "misclassification"#
)
model_grid
model_grid@model_ids
for( model_id in model_grid@model_ids )#
{#
    mse <- h2o.mse( h2o.getModel(model_id), valid = TRUE )#
    print( sprintf( "Test set MSE: %f", mse ) )#
}
?h2o.saveModel
getwd()
h2o.saveModel( object = model_grid, #
                             #path = , #
                             force = TRUE )
model_grid
class(model_grid)
model_grid@model_ids
for( model_id in model_grid@model_ids )#
{#
    mse <- h2o.mse( h2o.getModel(model_id), valid = TRUE )#
    print( sprintf( "Test set MSE: %f", mse ) )#
}
model_path <- h2o.saveModel( object = model_grid@model_ids[[4]], #
                             #path = , #
                             force = TRUE )
model_grid@model_ids[[4]]
model_path <- h2o.saveModel( object = h2o.getModel( model_grid@model_ids[[4]] ), #
                             #path = , #
                             force = TRUE )
?h2o.anomaly
h2o.getModel( model_grid@model_ids[[4]] )
getwd()
model_path <- h2o.saveModel( object = h2o.getModel( model_grid@model_ids[[4]] ), #
                             path = "/Users/ethen/Desktop", #
                             force = TRUE )
best <- h2o.getModel( model_grid@model_ids[[4]] )
h2o.predict( best, newdata = test )
?h2o.deeplearning
?h2o.grid
?h2o.deeplearning
saved_model <- h2o.loadModel(model_path)
saved_model
train_ecg <- h2o.importFile( path = "http://h2o-public-test-data.s3.amazonaws.com/smalldata/anomaly/ecg_discord_train.csv",#
                             header = FALSE, sep = "," )
test_ecg  <- h2o.importFile( path = "http://h2o-public-test-data.s3.amazonaws.com/smalldata/anomaly/ecg_discord_test.csv",#
                             header = FALSE, sep = "," )
names(train_ecg)
train_ecg
anomaly_model <- h2o.deeplearning( #
#
    training_frame = train_ecg,#
    x = names(train_ecg),#
    activation = "Tanh",#
    autoencoder = TRUE,#
    hidden = c( 50, 20, 50 ),#
    #sparse = TRUE,#
    l1 = 1e-4,#
    epochs = 100 #
)
h2o.anomaly(anomaly_model, test_ecg)
recon_error <- h2o.anomaly(anomaly_model, test_ecg)
recon_error <- as.data.frame(recon_error)
recon_error
plot.ts(recon_error)
test_ecg
h2o.predict( anomaly_model, test_ecg )
head(test_recon)
library(xgboost)
library(data.table)
data(Arthritis)
library(caret)
data(Arthritis)
library(Matrix)
data(Arthritis)
library(vcd)
?replace
library(vcd)
data(Arthritis)
head(Arthritis)
Arthritis
df <- data.table(Arthritis)
df
data.table(Arthritis, keep.rownames = F)
df <- data.table(Arthritis)
str(df)
df[ , AgeDiscrete := as.factor( round( Age / 10, 0 ) ) ]
df
df[ , AgeCat := as.factor( ifelse( Age > 30, "Old", "Young" ) ) ]
df
df[ , ID := NULL ]
df[,Treatment]
df[,1]
df[,Sex]
df[,"Sex"]
levels( df[ , Treatment ] )
?sparse.model.matrix
sparse.model.matrix( Improved~.-1, data = df )
df
sparse_matrix@Dimnames
sparse_matrix <- sparse.model.matrix( Improved~.-1, data = df )
sparse_matrix@Dimnames
head(sparse_matrix )
?xgb.importance
df[,Improved] == "Marked"
output_vector <- df[ , Improved ] == "Marked"
?xgboost
bst <- xgboost( data = sparse_matrix, label = output_vector, #
                max.depth = 4,#
                eta = 1, nround = 10, objective = "binary:logistic" )
importance <- xgb.importance(sparse_matrix@Dimnames[[2]], model = bst)#
head(importance)
importance_raw <- xgb.importance( sparse_matrix@Dimnames[[2]], #
                                  model = bst, #
                                  data = sparse_matrix, #
                                  label = output_vector )
importance_raw
importance_clean <- importance_raw[ , `:=`( Cover = NULL, Frequency = NULL ) ]
importance_clean <- importance_raw[ , `:=`( Cover = NULL, Frequence = NULL ) ]
importance_raw <- xgb.importance( sparse_matrix@Dimnames[[2]], #
                                  model = bst, #
                                  data = sparse_matrix, #
                                  label = output_vector )
importance_clean <- importance_raw[ , `:=`( Cover = NULL, Frequence = NULL ) ]
importance_clean
xgb.plot.importance( importance_matrix = importance_raw )
sessionInfo()
?xgboost
?chisq.test
xgb.plot.importance( importance_matrix = importance_raw )
?requireNamespace
xgb.plot.importance( importance_matrix = importance_raw )
df$Age
output_vector
chisq.test( df$Age, output_vector)
data(agaricus.train, package = "xgboost" )#
data(agaricus.test , package = "xgboost" )#
#
train <- agaricus.train#
test  <- agaricus.test
?xgboost
library(xgboost)
?xgboost
?getinfo
bst <- xgboost( data = train$data, label = train$label,#
                nround = 2, objective = "binary:logistic", #
                eval_metric = "auc" )
predict( bst, newdata = test$data )
test$data
xgboost( data = dtrain, #
         max.depth = 2, eta = 1, #
         nthread = 2, nround = 2, #
         objective = "binary:logistic" )
dtrain <- xgb.DMatrix( data = train$data, label = train$label )#
xgboost( data = dtrain, #
         max.depth = 2, eta = 1, #
         nthread = 2, nround = 2, #
         objective = "binary:logistic" )
result <- predict( bst, newdata = test$data )
mean( as.numeric( result > 0.5 ) != test$label )
as.numeric( result > 0.5 )
test$label
mean( as.numeric( result > 0.5 ) != test$label )
xgb.train
?xgb.train
xgb.plot.tree(model = bst)
?xgb.plot.tree
library(dplyr)#
library(ggplot2)#
library(data.table)#
setwd("/Users/ethen/Business-Analytics/3_fraud/data")#
#
load("sales.Rdata")#
sales <- data.table(sales)
na <- sales[ is.na(Quant) & is.na(Val), .( ID, Prod ) ]
sales <- sales[ !( is.na(Quant) & is.na(Val) ), ]
sales
sales[ , Uprice := Val / Quant ]#
#
# median price of every product ; #
# exclude the ones that are labeled as fraud !#
median_price <- sales[ Insp != "fraud", .( price = median( Uprice, na.rm = TRUE ) ), #
                       by = Prod ]#
#
# use the median unit price to replace products that contains missing values#
#
# use the original value / the replaced median price the estimate the missing quantity#
no_Quant <- is.na(sales$Quant)#
rows1  <- match( sales[ no_Quant, Prod ], median_price$Prod )#
value1 <- ceiling( sales[ no_Quant, Val ] / median_price$price[ rows1 ] ) %>% as.integer()#
sales[ no_Quant, Quant := value1 ]#
# does the same thing for products that have missing Val #
no_Val <- is.na(sales$Val)#
rows2  <- match( sales[ no_Val, Prod ], median_price$Prod )#
value2 <- ceiling( sales[ no_Val, Quant ] * median_price$price[ rows2 ] )#
sales[ no_Val, Val := value2 ]#
#
# re-calculate the uprice #
sales[ , Uprice := Val / Quant ]
salwa
sales
summary(sales)
load("sales.Rdata")#
sales <- data.table(sales)#
#
# considerable amount of missing values#
summary(sales)#
#
# distribution of fraud detected #
prop.table( table( sales$Insp ) )#
# which sales person generates the most ( and least ) sales #
top_sales <- sales[ , .( sum = sum( Val, na.rm = TRUE ) ), #
                         by = ID ][ order( sum, decreasing = TRUE ) ]#
#
# ---------------------------------------------------------------------#
#                   Data Preprocessing - Missing Values  #
# ---------------------------------------------------------------------                      #
#
# 1. having both values missing :#
# missing value only accounts for a small proportion #
# it seems reasonable to delete these records                        #
sum( is.na(sales$Quant) & is.na(sales$Val) )#
#
# check the distribution that's generated by each sales person and product#
# to see if it is contributed by particular sales person or product #
#
# sales person that have a larger proportion of transactions #
# with unknown values on both Val and Quant #
na <- sales[ is.na(Quant) & is.na(Val), .( ID, Prod ) ]#
#
# sales#
# ID and Quant is factor variables that have levels #
prop_s <- table(na$ID) / table(sales$ID)#
prop_s[ order( prop_s, decreasing = TRUE ) ][1:10]#
#
# products  #
prop_p <- table(na$Prod) / table(sales$Prod)#
prop_p[ order( prop_p, decreasing = TRUE ) ][1:10]#
#
# remove missing value #
sales <- sales[ !( is.na(Quant) & is.na(Val) ), ]#
# 2. analyze the remaining dataset, #
# with unknown values in either the quantity or the value of the transaction #
sum( is.na(sales$Quant) | is.na(sales$Val) )#
#
# products #
count_product <- sales[ , .( na_count = sum( is.na(Quant) ), #
                             count = .N ), by = Prod ]#
count_product[ , na_prop := na_count / count ][ order( na_prop, decreasing = TRUE ) ]#
#
# product id p2442 and p2443 that have all their transactions #
# with unknown values of quantity ;#
# remove them and re-set the factor level #
# nlevels(sales$Prod)#
sales <- sales[ !Prod %in% c( "p2442", "p2443" ), ]#
sales[ , Prod := factor(Prod) ]#
#
# unit price of any product should follow a near-normal distribution#
# there will be differences due different sales strategies carried out by sales person #
sales[ , Uprice := Val / Quant ]#
#
# median price of every product ; #
# exclude the ones that are labeled as fraud !#
median_price <- sales[ Insp != "fraud", .( price = median( Uprice, na.rm = TRUE ) ), #
                       by = Prod ]#
#
# use the median unit price to replace products that contains missing values#
#
# use the original value / the replaced median price the estimate the missing quantity#
no_Quant <- is.na(sales$Quant)#
rows1  <- match( sales[ no_Quant, Prod ], median_price$Prod )#
value1 <- ceiling( sales[ no_Quant, Val ] / median_price$price[ rows1 ] ) %>% as.integer()#
sales[ no_Quant, Quant := value1 ]#
# does the same thing for products that have missing Val #
no_Val <- is.na(sales$Val)#
rows2  <- match( sales[ no_Val, Prod ], median_price$Prod )#
value2 <- ceiling( sales[ no_Val, Quant ] * median_price$price[ rows2 ] )#
sales[ no_Val, Val := value2 ]#
#
# re-calculate the uprice #
sales[ , Uprice := Val / Quant ]
sales
summary(sales)
distribution <- sales[ Insp != "fraud", { #
#
    stats <- boxplot.stats(Uprice)$stats ; #
    list( median = stats[3], iqr = stats[4] - stats[2] )#
}, by = Prod ]
sales
table(sales$Prod)
data.table( table( sales$Prod) )[ order( N, decreasing = TRUE ) ]
prods <- split( sales$Uprice, sales$Prod )#
#
smalls <- which( table(sales$Prod) < 20 )
similar <- matrix(NA, length(smalls), 7, dimnames = list(names(smalls),#
     c("Simil", "ks.stat", "ks.p", "medP", "iqrP", "medS",#
         "iqrS")))
similar
head(smalls)
distribution
scaled
scaled <- distribution[ , lapply( .SD, scale ), .SDcols = -1 ]
scaled
smalls[1]
sales
table( sales$Insp )
distribution
scaled
length( unique(sales$Prod) )
smalls[1]
scaled
scaled[ smalls[1], ]
?scale
scale( scaled, scaled[ smalls[1], ], FALSE )
scaled
scaled[ smalls[1], ]
scaled
scale( scaled, scaled[ smalls[1], ], FALSE )
as.numeric(scaled[ smalls[1], ])
scale( scaled, as.numeric(scaled[ smalls[1], ]), FALSE )
?scale
d <- scale( scaled, as.numeric( scaled[ smalls[1], ] ), FALSE )
d^2
d^2 %*% rep( 1, ncol(d)
)
rowSums( d^2 )
rowSums( d^2 )[1:6]
head(d^2 %*% rep( 1, ncol(d) ))
d <- scale( scaled, center = as.numeric( scaled[ smalls[1], ] ), scale = FALSE )#
    # d <- scale(dms, dms[smalls[i], ], FALSE)#
    d <- sqrt( rowSums( d^2 ) )
d
d <- scale( scaled, center = as.numeric( scaled[ smalls[1], ] ), scale = FALSE )
head(d)
d <- sqrt( drop( d^2 %*% rep( 1, ncol(d) ) ) )
d
order(d)[2]
order(d)[1:6]
smalls[1]
stat <- ks.test( prods[[ smalls[1] ]], prods[[ order(d)[2] ]] )
stat
summary(stat)
stat$statistic
stat$p.value
distribution[ order(d)[2], ]
order(d)[2]
distribution[ smalls[1], ]
smalls[1]
most_similar <- order(d)[2]
most_similar
stat <- ks.test( prods[[ smalls[1] ]], prods[[ most_similar ]] )
stat
temp <- data.table( product = smalls[1],#
                        similar = most_similar,#
                        ksP = stat$p.value )
temp
smalls
distribution
smalls
nrow(smalls)
length(smalls)
distribution <- sales[ Insp != "fraud", { #
#
    stats <- boxplot.stats(Uprice)$stats ; #
    list( median = stats[3], iqr = stats[4] - stats[2] )#
}, by = Prod ]
distribution
KSTestCalculate <- function( distribution )#
{#
    # normalize the measurement #
    scaled <- distribution[ , lapply( .SD, scale ), .SDcols = -1 ]#
#
    # split the product's unit price by each product #
    prods <- split( sales$Uprice, sales$Prod )#
#
    # #
    smalls <- which( table(sales$Prod) < 20 )#
#
    info <- lapply( 1:length(smalls), function(i)#
    {#
        # compute the distance between the current project under analysis with #
        # all the other products#
        d <- scale( scaled, center = as.numeric( scaled[ smalls[i], ] ), scale = FALSE )#
        d <- sqrt( rowSums( d^2 ) )#
#
        # compute ks test between the second closest distance's product#
        # since the first one is itself#
        most_similar <- order(d)[2]#
        stat <- ks.test( prods[[ smalls[i] ]], prods[[ most_similar ]] )#
#
        # the current product ; the most similar product and the test's p-value#
        temp <- data.table( product = smalls[i],#
                            similar = most_similar,#
                            ksP     = stat$p.value )#
        return(temp)        #
    }) %>% rbindlist()#
#
    return(info)#
}#
#
info <- KSTestCalculate( distribution )
warnings()
scaled <- distribution[ , lapply( .SD, scale ), .SDcols = -1 ]#
#
    # split the product's unit price by each product #
    prods <- split( sales$Uprice, sales$Prod )#
#
    # obtain the product that have transaction count lower than the threshold (20)#
    smalls <- which( table(sales$Prod) < 20 )
info <- lapply( 1:length(smalls), function(i)#
    {#
        # compute the distance between the current project under analysis with #
        # all the other products#
        d <- scale( scaled, center = as.numeric( scaled[ smalls[i], ] ), scale = FALSE )#
        d <- sqrt( rowSums( d^2 ) )#
#
        # compute ks test between the second closest distance's product#
        # since the first one is itself#
        most_similar <- order(d)[2]#
        stat <- ks.test( prods[[ smalls[i] ]], prods[[ most_similar ]] )#
#
        # the current product ; the most similar product and the test's p-value#
        temp <- data.table( product = smalls[i],#
                            similar = most_similar,#
                            ksP     = stat$p.value )#
        return(temp)        #
    }) %>% rbindlist()
smalls
smalls[1]
scaled
d <- scale( scaled, center = as.numeric( scaled[ smalls[1], ] ), scale = FALSE )#
        d <- sqrt( rowSums( d^2 ) )#
#
        # compute ks test between the second closest distance's product#
        # since the first one is itself#
        most_similar <- order(d)[2]#
        stat <- ks.test( prods[[ smalls[1] ]], prods[[ most_similar ]] )#
#
        # the current product ; the most similar product and the test's p-value#
        temp <- data.table( product = smalls[1],#
                            similar = most_similar,#
                            ksP     = stat$p.value )
temp
d <- scale( scaled, center = as.numeric( scaled[ smalls[2], ] ), scale = FALSE )#
        d <- sqrt( rowSums( d^2 ) )#
#
        # compute ks test between the second closest distance's product#
        # since the first one is itself#
        most_similar <- order(d)[2]#
        stat <- ks.test( prods[[ smalls[2] ]], prods[[ most_similar ]] )#
#
        # the current product ; the most similar product and the test's p-value#
        temp <- data.table( product = smalls[2],#
                            similar = most_similar,#
                            ksP     = stat$p.value )
temp
most_similar
smalls[2]
most_similar
ks.test( prods[[ smalls[2] ]], prods[[ most_similar ]] )
prods[[ smalls[2] ]]
prods[[ most_similar ]]
?ks.test
info
distribution
sales
KSTestCalculate <- function( sales )#
{   #
    # calculate the median and IQR ( Q3 - Q1 ) for every product's unit price  #
    distribution <- sales[ Insp != "fraud", { #
#
        stats <- boxplot.stats(Uprice)$stats ; #
        list( median = stats[3], iqr = stats[4] - stats[2] )#
    }, by = Prod ]#
#
    # normalize the measurement #
    scaled <- distribution[ , lapply( .SD, scale ), .SDcols = -1 ]#
#
    # split the product's unit price by each product #
    prods <- split( sales$Uprice, sales$Prod )#
#
    # obtain the product that have transaction count lower than the threshold (20)#
    smalls <- which( table(sales$Prod) < 20 )#
#
    info <- lapply( 1:length(smalls), function(i)#
    {#
        # compute the distance between the current project under analysis with #
        # all the other products#
        d <- scale( scaled, center = as.numeric( scaled[ smalls[2], ] ), scale = FALSE )#
        d <- sqrt( rowSums( d^2 ) )#
#
        # compute ks test between the second closest distance's product#
        # since the first one is itself#
        most_similar <- order(d)[2]#
        stat <- ks.test( prods[[ smalls[2] ]], prods[[ most_similar ]] )#
#
        # the current product ; the most similar product and the test's p-value#
        temp <- data.table( product = smalls[2],#
                            similar = most_similar,#
                            ksP     = stat$p.value )#
        return(temp)        #
    }) %>% rbindlist()#
#
    return(info)#
}#
#
info <- KSTestCalculate( sales )
info
KSTestCalculate <- function( sales )#
{   #
    # calculate the median and IQR ( Q3 - Q1 ) for every product's unit price  #
    distribution <- sales[ Insp != "fraud", { #
#
        stats <- boxplot.stats(Uprice)$stats ; #
        list( median = stats[3], iqr = stats[4] - stats[2] )#
    }, by = Prod ]#
#
    # normalize the measurement #
    scaled <- distribution[ , lapply( .SD, scale ), .SDcols = -1 ]#
#
    # split the product's unit price by each product #
    prods <- split( sales$Uprice, sales$Prod )#
#
    # obtain the product that have transaction count lower than the threshold (20)#
    smalls <- which( table(sales$Prod) < 20 )#
#
    info <- lapply( 1:length(smalls), function(i)#
    {#
        # compute the distance between the current project under analysis with #
        # all the other products#
        d <- scale( scaled, center = as.numeric( scaled[ smalls[i], ] ), scale = FALSE )#
        d <- sqrt( rowSums( d^2 ) )#
#
        # compute ks test between the second closest distance's product#
        # since the first one is itself#
        most_similar <- order(d)[2]#
        stat <- ks.test( prods[[ smalls[i] ]], prods[[ most_similar ]] )#
#
        # the current product ; the most similar product and the test's p-value#
        temp <- data.table( product = smalls[i],#
                            similar = most_similar,#
                            ksP     = stat$p.value )#
        return(temp)        #
    }) %>% rbindlist()#
#
    return(info)#
}#
#
info <- KSTestCalculate( sales )
info
KSTestCalculate <- function( sales )#
{   #
    # calculate the median and IQR ( Q3 - Q1 ) for every product's unit price  #
    distribution <- sales[ Insp != "fraud", { #
#
        stats <- boxplot.stats(Uprice)$stats ; #
        list( median = stats[3], iqr = stats[4] - stats[2] )#
    }, by = Prod ]#
#
    # normalize the measurement ; exclude the product id  #
    scaled <- distribution[ , lapply( .SD, scale ), .SDcols = -1 ]#
#
    # split the product's unit price by each product #
    prods <- split( sales$Uprice, sales$Prod )#
#
    # obtain the product that have transaction count lower than the threshold (20)#
    smalls <- which( table(sales$Prod) < 20 )#
#
    similar <- lapply( 1:length(smalls), function(i)#
    {#
        # compute the distance between the current project under analysis with #
        # all the other products#
        d <- scale( scaled, center = as.numeric( scaled[ smalls[i], ] ), scale = FALSE )#
        d <- sqrt( rowSums( d^2 ) )#
#
        # compute ks test between the second closest distance's product#
        # since the first one is itself#
        most_similar <- order(d)[2]#
        stat <- ks.test( prods[[ smalls[i] ]], prods[[ most_similar ]] )#
#
        # the current product ; the most similar product and the test's p-value#
        temp <- data.table( product = smalls[i],#
                            similar = most_similar,#
                            ksP     = stat$p.value )#
        return(temp)        #
    }) %>% rbindlist()#
#
    return(similar)#
}#
#
similar <- KSTestCalculate( sales )
rm(info)
similar
sum( similar$ksP > 0.9 )
smalls
length(smalls)
sales
levels(sales$Prod)
levels(sales$Prod)[2826]

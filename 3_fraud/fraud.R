# fraud detection
library(dplyr)
library(ggplot2)
library(data.table)
setwd("/Users/ethen/Business-Analytics/3_fraud/data")

load("sales.Rdata")
sales <- data.table(sales)

# considerable amount of missing values
summary(sales)

# distribution of fraud detected 
prop.table( table( sales$Insp ) )


# which sales person generates the most ( and least ) sales 
top_sales <- sales[ , .( sum = sum( Val, na.rm = TRUE ) ), 
				         by = ID ][ order( sum, decreasing = TRUE ) ]

# ---------------------------------------------------------------------
#					Data Preprocessing - Missing Values  
# ---------------------------------------------------------------------				         

# 1. having both values missing :
# missing value only accounts for a small proportion 
# it seems reasonable to delete these records 				         
sum( is.na(sales$Quant) & is.na(sales$Val) )

# check the distribution that's generated by each sales person and product
# to see if it is contributed by particular sales person or product 

# sales person that have a larger proportion of transactions 
# with unknown values on both Val and Quant 
na <- sales[ is.na(Quant) & is.na(Val), .( ID, Prod ) ]

# sales
# ID and Quant is factor variables that have levels 
prop_s <- table(na$ID) / table(sales$ID)
prop_s[ order( prop_s, decreasing = TRUE ) ][1:10]

# products  
prop_p <- table(na$Prod) / table(sales$Prod)
prop_p[ order( prop_p, decreasing = TRUE ) ][1:10]

# remove missing value 
sales <- sales[ !( is.na(Quant) & is.na(Val) ), ]


# 2. analyze the remaining dataset, 
# with unknown values in either the quantity or the value of the transaction 
sum( is.na(sales$Quant) | is.na(sales$Val) )

# products 
count_product <- sales[ , .( na_count = sum( is.na(Quant) ), 
							 count = .N ), by = Prod ]
count_product[ , na_prop := na_count / count ][ order( na_prop, decreasing = TRUE ) ]

# product id p2442 and p2443 that have all their transactions 
# with unknown values of quantity ;
# remove them and re-set the factor level 
# nlevels(sales$Prod)
sales <- sales[ !Prod %in% c( "p2442", "p2443" ), ]
sales[ , Prod := factor(Prod) ]

# unit price of any product should follow a near-normal distribution
# there will be differences due different sales strategies carried out by sales person 
sales[ , Uprice := Val / Quant ]

# median price of every product ; 
# exclude the ones that are labeled as fraud !
median_price <- sales[ Insp != "fraud", .( price = median( Uprice, na.rm = TRUE ) ), 
					   by = Prod ]

# use the median unit price to replace products that contains missing values

# use the original value / the replaced median price the estimate the missing quantity
no_Quant <- is.na(sales$Quant)
rows1  <- match( sales[ no_Quant, Prod ], median_price$Prod )
value1 <- ceiling( sales[ no_Quant, Val ] / median_price$price[ rows1 ] ) %>% as.integer()
sales[ no_Quant, Quant := value1 ]


# does the same thing for products that have missing Val 
no_Val <- is.na(sales$Val)
rows2  <- match( sales[ no_Val, Prod ], median_price$Prod )
value2 <- ceiling( sales[ no_Val, Quant ] * median_price$price[ rows2 ] )
sales[ no_Val, Val := value2 ]

# re-calculate the uprice 
sales[ , Uprice := Val / Quant ]

# store the current clean-version of the data 
save( sales, file = "salesClean.Rdata" )


# ---------------------------------------------------------------------
#					Data Preprocessing - Few Transactions 
# ---------------------------------------------------------------------	

# for products that have very few transactions 
# it can be hard to determine whether they are a fraud or not
length( unique(sales$Prod) )
product <- data.table( table( sales$Prod) )[ order( N, decreasing = TRUE ) ]
quantile(product$N)

# thus we want to compare them 
# using statistical properties that summarize 
# the distributions. Two important properties of 
# continuous variables distributions are their central tendency and spread

# For each of the products that has less than 20 transactions, 
# we will search for the product with the most similar unit price distribution 
# and then use a Kolmogorov-Smirnov test to check if the similarity is statistically significant
# high p-value indicates that it is reasonable to assume that they come from the same distribution


KSTestCalculate <- function( sales )
{	
	# calculate the median and IQR ( Q3 - Q1 ) for every product's unit price  
	distribution <- sales[ Insp != "fraud", { 

		stats <- boxplot.stats(Uprice)$stats ; 
		list( median = stats[3], iqr = stats[4] - stats[2] )
	}, by = Prod ]

	# normalize the measurement ; exclude the product id  
	scaled <- distribution[ , lapply( .SD, scale ), .SDcols = -1 ]

	# split the product's unit price by each product 
	prods <- split( sales$Uprice, sales$Prod )

	# obtain the product that have transaction count lower than the threshold (20)
	smalls <- which( table(sales$Prod) < 20 )

	similar <- lapply( 1:length(smalls), function(i)
	{
		# compute the distance between the current project under analysis with 
		# all the other products
		d <- scale( scaled, center = as.numeric( scaled[ smalls[i], ] ), scale = FALSE )
		d <- sqrt( rowSums( d^2 ) )

		# compute ks test between the second closest distance's product
		# since the first one is itself
		most_similar <- order(d)[2]
		stat <- ks.test( prods[[ smalls[i] ]], prods[[ most_similar ]] )

		# the current product ; the most similar product and the test's p-value
		temp <- data.table( product = smalls[i],
							similar = most_similar,
							ksP 	= stat$p.value )
		return(temp)		
	}) %>% rbindlist()

	return(similar)
}

similar <- KSTestCalculate( sales )
sum( similar$ksP > 0.9 )


# book's code
# https://github.com/cran/DMwR/tree/master/R

# local outlier factor 
# http://www.cse.ust.hk/~leichen/courses/comp5331/lectures/LOF_Example.pdf



# smote 
# https://github.com/blacklab/nyan/blob/master/shared_modules/smote.py
# http://stackoverflow.com/questions/24101802/smote-fails-making-oversampling
# https://www.jair.org/media/953/live-953-2037-jair.pdf



